{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample thed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10., 10., 11.,  1.,  2., 15.,  6.,  0., 15.],\n",
      "        [ 2., 13.,  4., 13., 15.,  1.,  5., 12., 13.],\n",
      "        [10., 15.,  9.,  3.,  4., 15., 13.,  0.,  0.],\n",
      "        [ 7.,  2., 11., 12.,  8.,  2.,  2.,  8.,  7.],\n",
      "        [ 8.,  0.,  5.,  3.,  5.,  6., 11.,  3.,  6.]])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import wandb\n",
    "import torch\n",
    "import numpy as np\n",
    "from model import ModelArgs, MLPEncoderArgs, Transformer, MLPEncoder, MLLMTransformer, TransformerEncoderArgs, TransformerEncoder, CNNEncoderArgs,CNNEncoder\n",
    "from dataset import ICLDataset, MMDataset, get_mus_label_class, generate_input_seqs, generate_input_seqs_mm_v1,get_mm_mus_label_class\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Sample the data\n",
    "K1 = 8192\n",
    "K2=512\n",
    "N=8\n",
    "L1 = 32\n",
    "L2 = 16\n",
    "D1 = 64\n",
    "D2 = 512\n",
    "S = 5\n",
    "B=1\n",
    "alpha1 = 0.0\n",
    "alpha2 = 0.0\n",
    "eps1 = 0.1\n",
    "eps2 = 0.1\n",
    "no_repeats = False\n",
    "rope = True\n",
    "rope_theta = 10000\n",
    "P1 =1.0/(np.arange(1,K1+1)**alpha1)\n",
    "P1 = P1/np.sum(P1)\n",
    "P2 =1.0/(np.arange(1,K2+1)**alpha2)\n",
    "P2 = P2/np.sum(P2)\n",
    "mus_label_m1, mus_class_m1, labels_class_m1, mus_label_m2, mus_class_m2, labels_class_m2, mapping_m2_to_m1 = get_mm_mus_label_class(K1=K1,K2=K2,L1=L1,L2=L2,D1=D1,D2=D2)\n",
    "inputs_mm, inputs_2, labels, label_sequences = generate_input_seqs_mm_v1(mus_label_m1=mus_label_m1, mus_class_m1=mus_class_m1, mus_label_m2=mus_label_m2, mus_class_m2=mus_class_m2, labels_class_m2=labels_class_m2, mapping_m2_to_m1=mapping_m2_to_m1, N=N,S=S,eps1=eps1,eps2=eps2, P1 = P1, P2 = P2, B = B, p_B = 1, p_C = 1, no_repeats = no_repeats, seq_labels=True)\n",
    "print(label_sequences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of the feature before and after encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aoq609/miniconda3/envs/icl/lib/python3.13/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import wandb\n",
    "import torch\n",
    "import numpy as np\n",
    "from model import ModelArgs, MLPEncoderArgs, Transformer, MLPEncoder, MLLMTransformer, TransformerEncoderArgs, TransformerEncoder, CNNEncoderArgs,CNNEncoder\n",
    "from dataset import ICLDataset, MMDataset, get_mus_label_class, generate_input_seqs, generate_input_seqs_mm_v1,get_mm_mus_label_class\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "SEED = 0\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "K1 = 8192\n",
    "K2=512\n",
    "eps0=0.5\n",
    "D2=512\n",
    "D1=64\n",
    "dir = os.getcwd()\n",
    "ckpt_path_enc = f\"{dir}/outs_encoder_transformer/K{K2}_eps{eps0}_feat_dim{D2}_input_dim128_output_dim{D1//2}_num_layers2_num_heads1_niter50000/seed_0/ckpt_49999.pt\"\n",
    "model_args_enc = TransformerEncoderArgs(\n",
    "            feat_dim=D2,\n",
    "            input_dim=128,\n",
    "            output_dim=D1//2,\n",
    "            num_classes=K2,\n",
    "            num_layers=2,\n",
    "            num_heads=1\n",
    "        )\n",
    "Encoder = TransformerEncoder(model_args_enc)\n",
    "Encoder.load_state_dict(torch.load(ckpt_path_enc), strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2_raw = torch.FloatTensor(mus_class_m2)\n",
    "x2_vit = Encoder.extract_features(x2_raw).squeeze(1)\n",
    "x1 = torch.FloatTensor(mus_class_m1)[mapping_m2_to_m1[:,0]]\n",
    "def l2_norm(v): return v / v.norm(dim=-1, keepdim=True).clamp_min(1e-9)\n",
    "\n",
    "x1_norm      = l2_norm(x1)\n",
    "x2_encoder_proj_norm  = l2_norm(x2_raw)\n",
    "x2_vit_norm = l2_norm(x2_vit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare x2_raw with x2_vit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 Global geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 singular values\n",
      " raw : [44.54999923706055, 44.275001525878906, 44.17100143432617, 43.595001220703125, 43.33700180053711, 43.20899963378906, 42.939998626708984, 42.729000091552734, 42.680999755859375, 42.29999923706055] \n",
      " vit : [37.85599899291992, 34.77799987792969, 33.06999969482422, 32.689998626708984, 31.996999740600586, 31.75, 30.42799949645996, 29.457000732421875, 29.24799919128418, 28.30299949645996]\n",
      "eff-rank raw = 412.1483154296875 \n",
      " vit = 25.776538848876953\n",
      "isotropy raw = 0.43117275834083557 \n",
      " vit = 0.5104237794876099\n"
     ]
    }
   ],
   "source": [
    "# Spectrum\n",
    "Xraw = (x2_raw - x2_raw.mean(0)) / x2_raw.std(0, unbiased=False).clamp_min(1e-9)\n",
    "Xvit = (x2_vit - x2_vit.mean(0)) / x2_vit.std(0, unbiased=False).clamp_min(1e-9)\n",
    "vals_raw = torch.linalg.svdvals(Xraw)\n",
    "vals_vit = torch.linalg.svdvals(Xvit)\n",
    "print(\n",
    "    \"Top-5 singular values\\n raw :\",\n",
    "    (torch.round(vals_raw[:10],  decimals=3)).tolist(),\n",
    "    \"\\n vit :\",\n",
    "    (torch.round(vals_vit[:10],  decimals=3)).tolist()\n",
    ")\n",
    "# Effective rank (lower ⇢ fewer useful dims)\n",
    "def eff_rank(v): \n",
    "    s=v/v.sum()\n",
    "    return torch.exp(-(s*torch.log(s)).sum()) \n",
    "print(\"eff-rank raw =\", eff_rank(vals_raw).item(), \"\\n vit =\", eff_rank(vals_vit).item())\n",
    "\n",
    "# Isotropy ratio (≈trace / λ₁; bigger = flatter)\n",
    "iso = lambda v: v.sum()/(v.max()*len(v)) \n",
    "print(\"isotropy raw =\", iso(vals_raw).item(), \"\\n vit =\", iso(vals_vit).item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 Pair-wise structure (cluster separation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x2_raw_norm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# cosine distance matrix (fits on GPU with K2=512)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     D_raw = \u001b[32m1\u001b[39m - \u001b[43mx2_raw_norm\u001b[49m @ x2_raw_norm.T          \u001b[38;5;66;03m# [K2,K2], 0 means vectors are very similar (angle = 0°), 1 means vectors are orthogonal (angle = 90°)\u001b[39;00m\n\u001b[32m      4\u001b[39m     D_vit = \u001b[32m1\u001b[39m - x2_vit_norm @ x2_vit_norm.T\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstats\u001b[39m(D):\n",
      "\u001b[31mNameError\u001b[39m: name 'x2_raw_norm' is not defined"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # cosine distance matrix (fits on GPU with K2=512)\n",
    "    D_raw = 1 - x2_raw_norm @ x2_raw_norm.T          # [K2,K2], 0 means vectors are very similar (angle = 0°), 1 means vectors are orthogonal (angle = 90°)\n",
    "    D_vit = 1 - x2_vit_norm @ x2_vit_norm.T\n",
    "\n",
    "def stats(D):\n",
    "    tri = D.triu(1)                      # upper triangle, no diag\n",
    "    return tri[tri>0].mean(), tri[tri>0].std()\n",
    "\n",
    "mu_raw,  sd_raw  = stats(D_raw)\n",
    "mu_vit,  sd_vit  = stats(D_vit) \n",
    "print(f\"mean cosine-dist  raw={mu_raw:.3f}±{sd_raw:.3f}   vit={mu_vit:.3f}±{sd_vit:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⟨cos⟩ after Procrustes, raw: 0.17050622403621674 vit: 0.17902471125125885\n"
     ]
    }
   ],
   "source": [
    "from torch.linalg import svd\n",
    "# ❷ Compute cross-covariance and SVD\n",
    "M_raw = x2_raw_norm.T @ x1_norm                          # [D2, D1]\n",
    "U_raw, _, Vt_raw = svd(M_raw, full_matrices=False)   # U:[D2,r], Vt:[r,D1], r = min(D1,D2)\n",
    "M_vit = x2_vit_norm.T @ x1_norm                          # [D2, D1]\n",
    "U_vit, _, Vt_vit = svd(M_vit, full_matrices=False)   # U:[D2,r], Vt:[r,D1], r = min(D1,D2)\n",
    "\n",
    "# ❸ Best orthogonal map W*:      X2 · W* ≈ X1\n",
    "Wstar_raw = U_raw @ Vt_raw                          # [D2, D1]\n",
    "Wstar_vit = U_vit @ Vt_vit                          # [D2, D1]\n",
    "\n",
    "# ❹ Apply and evaluate\n",
    "X2_to_X1_raw = x2_raw_norm @ Wstar_raw                  # [K2, D1]\n",
    "X2_to_X1_vit = x2_vit_norm @ Wstar_vit                  # [K2, D1]\n",
    "paired_cos_raw = torch.sum(x1_norm * X2_to_X1_raw, dim=-1)  # cosine for every class\n",
    "paired_cos_vit = torch.sum(x1_norm * X2_to_X1_vit, dim=-1)  # cosine for every class\n",
    "print(\"⟨cos⟩ after Procrustes, raw:\", paired_cos_raw.mean().item(), \"vit:\", paired_cos_vit.mean().item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of the feature w/wo encoder after projector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the proj only model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MMTransformer(\n",
       "  (layers): ModuleList(\n",
       "    (0-1): 2 x TransformerBlock(\n",
       "      (attn): Attention(\n",
       "        (wq): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (wk): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (wv): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (act): SiLU()\n",
       "      )\n",
       "      (attn_norm): RMSNorm()\n",
       "      (mlp_norm): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       "  (out): Linear(in_features=64, out_features=32, bias=False)\n",
       "  (projector): Projector(\n",
       "    (fc1): Linear(in_features=512, out_features=64, bias=False)\n",
       "    (act): GELU(approximate='none')\n",
       "    (fc2): Linear(in_features=64, out_features=64, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import wandb\n",
    "import torch\n",
    "import numpy as np\n",
    "from model import ModelArgs, Transformer, MMTransformer\n",
    "from dataset import ICLDataset, MMDataset, get_mus_label_class, generate_input_seqs, generate_input_seqs_mm_v1,get_mm_mus_label_class\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "K1 = 8192\n",
    "K2=512\n",
    "eps0=0.5\n",
    "D2=512\n",
    "D1=64\n",
    "N=8\n",
    "model_args_stage2 = ModelArgs(\n",
    "        m1_dim=D1,\n",
    "        m2_dim=D2,\n",
    "        dim=D1,\n",
    "        n_layers=2,\n",
    "        n_heads=1,\n",
    "        n_labels=32,\n",
    "        max_position_embeddings=3*N+1,\n",
    "        rope_theta=10000,\n",
    "        mlp_bias=True,\n",
    "        rms_norm=True,\n",
    "        rope=True,\n",
    "        norm_eps=1e-5,\n",
    "        L_pos=64\n",
    "    )\n",
    "model_stage2 = MMTransformer(model_args_stage2)\n",
    "ckpt_path_stage2 = \"/home/aoq609/ICL/outs_torch/K1_8192_K2_512_N8_D1_64_D2_512_L1_32_L2_16_alpha1_0.0_alpha2_0.0_B2_pB1.0_pC0.0_eps1_0.1_eps2_0.1_no_repeatsFalse_rope_True_rope_theta10000_freeze_layersFalse_n_heads1_n_layers2_rms_normTrue_optimizerSGD_niters80000_n_epochs1/seed_0/ckpt_20000.pt\"\n",
    "model_stage2.load_state_dict(torch.load(ckpt_path_stage2), strict=False)\n",
    "model_stage2.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load stage 3 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLLMTransformer(\n",
      "  (layers): ModuleList(\n",
      "    (0-1): 2 x TransformerBlock(\n",
      "      (attn): Attention(\n",
      "        (wq): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (wk): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (wv): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (rotary_emb): RotaryEmbedding()\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (act): SiLU()\n",
      "      )\n",
      "      (attn_norm): RMSNorm()\n",
      "      (mlp_norm): RMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): RMSNorm()\n",
      "  (out): Linear(in_features=64, out_features=32, bias=False)\n",
      "  (projector): Projector(\n",
      "    (fc1): Linear(in_features=32, out_features=64, bias=False)\n",
      "    (act): GELU(approximate='none')\n",
      "    (fc2): Linear(in_features=64, out_features=64, bias=False)\n",
      "  )\n",
      "  (encoder): TransformerEncoder(\n",
      "    (patch_embed): Linear(in_features=128, out_features=32, bias=True)\n",
      "    (encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-1): 2 x TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=32, out_features=160, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=160, out_features=32, bias=True)\n",
      "          (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (classifier): Linear(in_features=32, out_features=512, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aoq609/miniconda3/envs/icl/lib/python3.13/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import wandb\n",
    "import torch\n",
    "import numpy as np\n",
    "from model import ModelArgs, MLPEncoderArgs, Transformer, MLPEncoder, MLLMTransformer, TransformerEncoderArgs, TransformerEncoder, CNNEncoderArgs,CNNEncoder\n",
    "from dataset import ICLDataset, MMDataset, get_mus_label_class, generate_input_seqs, generate_input_seqs_mm_v1,get_mm_mus_label_class\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "model_args_mm = ModelArgs(\n",
    "        m1_dim=D1,\n",
    "        m2_dim=D1//2,\n",
    "        dim=D1,\n",
    "        n_layers=2,\n",
    "        n_heads=1,\n",
    "        n_labels=32,\n",
    "        max_position_embeddings=3*N+1,\n",
    "        rope_theta=10000,\n",
    "        mlp_bias=True,\n",
    "        rms_norm=True,\n",
    "        rope=True,\n",
    "        norm_eps=1e-5,\n",
    "        L_pos=64\n",
    "    )\n",
    "model_args_enc = TransformerEncoderArgs(\n",
    "            feat_dim=D2,\n",
    "            input_dim=128,\n",
    "            output_dim=D1//2,\n",
    "            num_classes=K2,\n",
    "            num_layers=2,\n",
    "            num_heads=1\n",
    "        )\n",
    "Encoder = TransformerEncoder(model_args_enc)\n",
    "K1 = 8192\n",
    "K2=512\n",
    "eps0=0.5\n",
    "D2=512\n",
    "D1=64\n",
    "dir = os.getcwd()\n",
    "ckpt_path_enc = f\"{dir}/outs_encoder_transformer/K{K2}_eps{eps0}_feat_dim{D2}_input_dim128_output_dim{D1//2}_num_layers2_num_heads1_niter50000/seed_0/ckpt_49999.pt\"\n",
    "Encoder.load_state_dict(torch.load(ckpt_path_enc), strict=True)\n",
    "model_stage3 = MLLMTransformer(model_args_mm)\n",
    "model_stage3.init_encoder(Encoder)\n",
    "ckpt_path = \"/home/aoq609/ICL/outs_torch/K1_8192_K2_512_N8_D1_64_D2_512_L1_32_L2_16_alpha1_0.0_alpha2_0.0_B2_pB1.0_pC0.0_eps00.5_eps1_0.1_eps2_0.1_no_repeatsFalse_rope_True_encoder_transformer_freeze_layersFalse_freeze_encoderFalse_n_heads1_n_layers2_niters150000/seed_0/ckpt_140000.pt\"\n",
    "model_stage3.load_state_dict(torch.load(ckpt_path),strict=False)\n",
    "model_stage3.eval()\n",
    "print(model_stage3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 256)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping_m2_to_m1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2_raw = torch.FloatTensor(mus_class_m2)\n",
    "x2_proj = model_stage2.projector(x2_raw)\n",
    "x2_encoder_proj = model_stage3.projector(model_stage3.encoder.extract_features(x2_raw)).squeeze(1)\n",
    "# x1 = torch.FloatTensor(mus_class_m1)[mapping_m2_to_m1[:,0]]\n",
    "mus_class_m1_t = torch.as_tensor(mus_class_m1, dtype=torch.float32)     # [K1, D1]\n",
    "\n",
    "x1 = torch.stack([\n",
    "        mus_class_m1_t[torch.as_tensor(idxs)].mean(dim=0)               # mean along class axis\n",
    "        for idxs in mapping_m2_to_m1                                    # one row per modality-2 class\n",
    "    ])                                                                  # -> [K2, D1]\n",
    "def l2_norm(v): return v / v.norm(dim=-1, keepdim=True).clamp_min(1e-9)\n",
    "\n",
    "# x1_norm      = l2_norm(x1).detach()\n",
    "# x2_proj_norm = l2_norm(x2_proj).detach()\n",
    "# x2_encoder_proj_norm = l2_norm(x2_encoder_proj).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1_norm.shape torch.Size([512, 64])\n",
      "x2_proj_norm.shape torch.Size([512, 64])\n",
      "x2_encoder_proj_norm.shape torch.Size([512, 64])\n"
     ]
    }
   ],
   "source": [
    "print(\"x1_norm.shape\", x1_norm.shape)\n",
    "print(\"x2_proj_norm.shape\", x2_proj_norm.shape)\n",
    "print(\"x2_encoder_proj_norm.shape\", x2_encoder_proj_norm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Effective Rank (energy-based, centered) ===\n",
      "x1_norm                 : 13.615\n",
      "x2_raw                  : 310.255\n",
      "x2_proj_norm            : 10.101\n",
      "x2_encoder_proj_norm    : 17.148\n",
      "\n",
      "=== Effective Rank (amplitude-based, centered) ===\n",
      "x1_norm                 : 14.621\n",
      "x2_raw                  : 411.954\n",
      "x2_proj_norm            : 29.342\n",
      "x2_encoder_proj_norm    : 33.910\n",
      "x1_norm                  shape = (512, 64)\n",
      "x2_raw                   shape = (512, 512)\n",
      "x2_proj_norm             shape = (512, 64)\n",
      "x2_encoder_proj_norm     shape = (512, 64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "def effective_rank(X: torch.Tensor, center: bool = True, use_squared: bool = True, eps: float = 1e-12):\n",
    "    \"\"\"\n",
    "    Effective rank of matrix X via entropy of singular-value distribution.\n",
    "      - center: subtract column mean before SVD (recommended for embeddings).\n",
    "      - use_squared: p_i ∝ σ_i^2 (energy). If False, p_i ∝ σ_i (amplitude).\n",
    "    Returns: dict with e_rank, entropy, p (distribution), and singular values.\n",
    "    \"\"\"\n",
    "    assert X.ndim == 2, \"X must be 2D [n_samples, n_features]\"\n",
    "    Xc = X - X.mean(dim=0, keepdim=True) if center else X\n",
    "\n",
    "    # Singular values (no need for full U,V)\n",
    "    s = torch.linalg.svdvals(Xc)  # shape [min(n,d)]\n",
    "\n",
    "    if use_squared:\n",
    "        w = s**2\n",
    "    else:\n",
    "        w = s\n",
    "\n",
    "    w = torch.clamp(w, min=eps)\n",
    "    p = w / w.sum()\n",
    "\n",
    "    H = -(p * torch.log(p)).sum()          # natural log\n",
    "    e_rank = torch.exp(H)                  # effective rank\n",
    "    return {\"e_rank\": e_rank.item(), \"H\": H.item(), \"p\": p, \"s\": s}\n",
    "\n",
    "# --- Compute for your tensors ---\n",
    "tensors = {\n",
    "    \"x1_norm\": x1_norm.detach().cpu(),\n",
    "    \"x2_raw\": x2_raw.detach().cpu(),\n",
    "    \"x2_proj_norm\": x2_proj_norm.detach().cpu(),\n",
    "    \"x2_encoder_proj_norm\": x2_encoder_proj_norm.detach().cpu(),\n",
    "}\n",
    "\n",
    "print(\"=== Effective Rank (energy-based, centered) ===\")\n",
    "results = {}\n",
    "for name, X in tensors.items():\n",
    "    res = effective_rank(X, center=True, use_squared=True)\n",
    "    results[name] = res[\"e_rank\"]\n",
    "    print(f\"{name:24s}: {res['e_rank']:.3f}\")\n",
    "\n",
    "# (Optional) also report the amplitude-based variant for completeness\n",
    "print(\"\\n=== Effective Rank (amplitude-based, centered) ===\")\n",
    "for name, X in tensors.items():\n",
    "    res = effective_rank(X, center=True, use_squared=False)\n",
    "    print(f\"{name:24s}: {res['e_rank']:.3f}\")\n",
    "\n",
    "# (Optional) sanity: show matrix shapes\n",
    "for name, X in tensors.items():\n",
    "    print(f\"{name:24s} shape = {tuple(X.shape)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Effective Rank (energy-based, centered) ===\n",
      "x1                      : 13.524\n",
      "x2_raw                  : 310.255\n",
      "x2_proj                 : 8.654\n",
      "x2_encoder_proj         : 16.556\n",
      "\n",
      "=== Effective Rank (amplitude-based, centered) ===\n",
      "x1                      : 14.589\n",
      "x2_raw                  : 411.954\n",
      "x2_proj                 : 27.071\n",
      "x2_encoder_proj         : 33.195\n",
      "x1                       shape = (512, 64)\n",
      "x2_raw                   shape = (512, 512)\n",
      "x2_proj                  shape = (512, 64)\n",
      "x2_encoder_proj          shape = (512, 64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "def effective_rank(X: torch.Tensor, center: bool = True, use_squared: bool = True, eps: float = 1e-12):\n",
    "    \"\"\"\n",
    "    Effective rank of matrix X via entropy of singular-value distribution.\n",
    "      - center: subtract column mean before SVD (recommended for embeddings).\n",
    "      - use_squared: p_i ∝ σ_i^2 (energy). If False, p_i ∝ σ_i (amplitude).\n",
    "    Returns: dict with e_rank, entropy, p (distribution), and singular values.\n",
    "    \"\"\"\n",
    "    assert X.ndim == 2, \"X must be 2D [n_samples, n_features]\"\n",
    "    Xc = X - X.mean(dim=0, keepdim=True) if center else X\n",
    "\n",
    "    # Singular values (no need for full U,V)\n",
    "    s = torch.linalg.svdvals(Xc)  # shape [min(n,d)]\n",
    "\n",
    "    if use_squared:\n",
    "        w = s**2\n",
    "    else:\n",
    "        w = s\n",
    "\n",
    "    w = torch.clamp(w, min=eps)\n",
    "    p = w / w.sum()\n",
    "\n",
    "    H = -(p * torch.log(p)).sum()          # natural log\n",
    "    e_rank = torch.exp(H)                  # effective rank\n",
    "    return {\"e_rank\": e_rank.item(), \"H\": H.item(), \"p\": p, \"s\": s}\n",
    "\n",
    "# --- Compute for your tensors ---\n",
    "tensors = {\n",
    "    \"x1\": x1.detach().cpu(),\n",
    "    \"x2_raw\": x2_raw.detach().cpu(),\n",
    "    \"x2_proj\": x2_proj.detach().cpu(),\n",
    "    \"x2_encoder_proj\": x2_encoder_proj.detach().cpu(),\n",
    "}\n",
    "\n",
    "print(\"=== Effective Rank (energy-based, centered) ===\")\n",
    "results = {}\n",
    "for name, X in tensors.items():\n",
    "    res = effective_rank(X, center=True, use_squared=True)\n",
    "    results[name] = res[\"e_rank\"]\n",
    "    print(f\"{name:24s}: {res['e_rank']:.3f}\")\n",
    "\n",
    "# (Optional) also report the amplitude-based variant for completeness\n",
    "print(\"\\n=== Effective Rank (amplitude-based, centered) ===\")\n",
    "for name, X in tensors.items():\n",
    "    res = effective_rank(X, center=True, use_squared=False)\n",
    "    print(f\"{name:24s}: {res['e_rank']:.3f}\")\n",
    "\n",
    "# (Optional) sanity: show matrix shapes\n",
    "for name, X in tensors.items():\n",
    "    print(f\"{name:24s} shape = {tuple(X.shape)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CKA with x1_norm ===\n",
      "x1_norm vs x2_raw               : 0.1097\n",
      "x1_norm vs x2_proj_norm         : 0.3793\n",
      "x1_norm vs x2_encoder_proj_norm : 0.0373\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def gram_linear(X: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Compute linear kernel Gram matrix.\"\"\"\n",
    "    return X @ X.T\n",
    "\n",
    "def center_gram(G: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Center the Gram matrix (like in HSIC).\"\"\"\n",
    "    n = G.size(0)\n",
    "    H = torch.eye(n, device=G.device) - torch.ones((n,n), device=G.device)/n\n",
    "    return H @ G @ H\n",
    "\n",
    "def cka(X: torch.Tensor, Y: torch.Tensor, center: bool = True) -> float:\n",
    "    \"\"\"\n",
    "    Compute linear CKA between two sets of representations.\n",
    "    X: [n, d1], Y: [n, d2]\n",
    "    \"\"\"\n",
    "    # ensure same number of rows (samples)\n",
    "    assert X.size(0) == Y.size(0), \"X and Y must have same number of samples\"\n",
    "\n",
    "    Gx = gram_linear(X)\n",
    "    Gy = gram_linear(Y)\n",
    "\n",
    "    if center:\n",
    "        Gx = center_gram(Gx)\n",
    "        Gy = center_gram(Gy)\n",
    "\n",
    "    hsic = (Gx * Gy).sum()\n",
    "    norm_x = torch.sqrt((Gx * Gx).sum())\n",
    "    norm_y = torch.sqrt((Gy * Gy).sum())\n",
    "    return (hsic / (norm_x * norm_y)).item()\n",
    "\n",
    "# --- compute CKA ---\n",
    "pairs = {\n",
    "    \"x1_norm vs x2_raw\": (x1_norm, x2_raw),\n",
    "    \"x1_norm vs x2_proj_norm\": (x1_norm, x2_proj_norm),\n",
    "    \"x1_norm vs x2_encoder_proj_norm\": (x1_norm, x2_encoder_proj_norm),\n",
    "}\n",
    "\n",
    "print(\"=== CKA with x1_norm ===\")\n",
    "for name, (X, Y) in pairs.items():\n",
    "    val = cka(X.detach(), Y.detach())\n",
    "    print(f\"{name:32s}: {val:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CKA with x1 ===\n",
      "x1 vs x2_raw                    : 0.1089\n",
      "x1 vs x2_proj                   : 0.3241\n",
      "x1 vs x2_encoder_proj           : 0.0349\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def gram_linear(X: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Compute linear kernel Gram matrix.\"\"\"\n",
    "    return X @ X.T\n",
    "\n",
    "def center_gram(G: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Center the Gram matrix (like in HSIC).\"\"\"\n",
    "    n = G.size(0)\n",
    "    H = torch.eye(n, device=G.device) - torch.ones((n,n), device=G.device)/n\n",
    "    return H @ G @ H\n",
    "\n",
    "def cka(X: torch.Tensor, Y: torch.Tensor, center: bool = True) -> float:\n",
    "    \"\"\"\n",
    "    Compute linear CKA between two sets of representations.\n",
    "    X: [n, d1], Y: [n, d2]\n",
    "    \"\"\"\n",
    "    # ensure same number of rows (samples)\n",
    "    assert X.size(0) == Y.size(0), \"X and Y must have same number of samples\"\n",
    "\n",
    "    Gx = gram_linear(X)\n",
    "    Gy = gram_linear(Y)\n",
    "\n",
    "    if center:\n",
    "        Gx = center_gram(Gx)\n",
    "        Gy = center_gram(Gy)\n",
    "\n",
    "    hsic = (Gx * Gy).sum()\n",
    "    norm_x = torch.sqrt((Gx * Gx).sum())\n",
    "    norm_y = torch.sqrt((Gy * Gy).sum())\n",
    "    return (hsic / (norm_x * norm_y)).item()\n",
    "\n",
    "# --- compute CKA ---\n",
    "pairs = {\n",
    "    \"x1 vs x2_raw\": (x1, x2_raw),\n",
    "    \"x1 vs x2_proj\": (x1, x2_proj),\n",
    "    \"x1 vs x2_encoder_proj\": (x1, x2_encoder_proj),\n",
    "}\n",
    "\n",
    "print(\"=== CKA with x1 ===\")\n",
    "for name, (X, Y) in pairs.items():\n",
    "    val = cka(X.detach(), Y.detach())\n",
    "    print(f\"{name:32s}: {val:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2(x2_proj, x1)      = 0.0778\n",
      "L2(x2_encoder_proj, x1) = 1.4049\n",
      "L2(x2_proj, x2_encoder_proj) = 1.4071\n"
     ]
    }
   ],
   "source": [
    "def l2_distance(a, b):\n",
    "    return (a - b).norm(p=2, dim=1).mean().item()\n",
    "\n",
    "dist_proj_vs_x1      = l2_distance(x2_proj, x1)\n",
    "dist_encoder_vs_x1   = l2_distance(x2_encoder_proj, x1)\n",
    "dist_proj_vs_encoder = l2_distance(x2_proj, x2_encoder_proj)\n",
    "\n",
    "print(f\"L2(x2_proj, x1)      = {dist_proj_vs_x1:.4f}\")\n",
    "print(f\"L2(x2_encoder_proj, x1) = {dist_encoder_vs_x1:.4f}\")\n",
    "print(f\"L2(x2_proj, x2_encoder_proj) = {dist_proj_vs_encoder:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
