{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample thed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10., 10., 11.,  1.,  2., 15.,  6.,  0., 15.],\n",
      "        [ 2., 13.,  4., 13., 15.,  1.,  5., 12., 13.],\n",
      "        [10., 15.,  9.,  3.,  4., 15., 13.,  0.,  0.],\n",
      "        [ 7.,  2., 11., 12.,  8.,  2.,  2.,  8.,  7.],\n",
      "        [ 8.,  0.,  5.,  3.,  5.,  6., 11.,  3.,  6.]])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import wandb\n",
    "import torch\n",
    "import numpy as np\n",
    "from model import ModelArgs, MLPEncoderArgs, Transformer, MLPEncoder, MLLMTransformer, TransformerEncoderArgs, TransformerEncoder, CNNEncoderArgs,CNNEncoder\n",
    "from dataset import ICLDataset, MMDataset, get_mus_label_class, generate_input_seqs, generate_input_seqs_mm_v1,get_mm_mus_label_class\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Sample the data\n",
    "K1 = 8192\n",
    "K2=512\n",
    "N=8\n",
    "L1 = 32\n",
    "L2 = 16\n",
    "D1 = 64\n",
    "D2 = 512\n",
    "S = 5\n",
    "B=1\n",
    "alpha1 = 0.0\n",
    "alpha2 = 0.0\n",
    "eps1 = 0.1\n",
    "eps2 = 0.1\n",
    "no_repeats = False\n",
    "rope = True\n",
    "rope_theta = 10000\n",
    "P1 =1.0/(np.arange(1,K1+1)**alpha1)\n",
    "P1 = P1/np.sum(P1)\n",
    "P2 =1.0/(np.arange(1,K2+1)**alpha2)\n",
    "P2 = P2/np.sum(P2)\n",
    "mus_label_m1, mus_class_m1, labels_class_m1, mus_label_m2, mus_class_m2, labels_class_m2, mapping_m2_to_m1 = get_mm_mus_label_class(K1=K1,K2=K2,L1=L1,L2=L2,D1=D1,D2=D2)\n",
    "inputs_mm, inputs_2, labels, label_sequences = generate_input_seqs_mm_v1(mus_label_m1=mus_label_m1, mus_class_m1=mus_class_m1, mus_label_m2=mus_label_m2, mus_class_m2=mus_class_m2, labels_class_m2=labels_class_m2, mapping_m2_to_m1=mapping_m2_to_m1, N=N,S=S,eps1=eps1,eps2=eps2, P1 = P1, P2 = P2, B = B, p_B = 1, p_C = 1, no_repeats = no_repeats, seq_labels=True)\n",
    "print(label_sequences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of the feature before and after encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aoq609/miniconda3/envs/icl/lib/python3.13/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import wandb\n",
    "import torch\n",
    "import numpy as np\n",
    "from model import ModelArgs, MLPEncoderArgs, Transformer, MLPEncoder, MLLMTransformer, TransformerEncoderArgs, TransformerEncoder, CNNEncoderArgs,CNNEncoder\n",
    "from dataset import ICLDataset, MMDataset, get_mus_label_class, generate_input_seqs, generate_input_seqs_mm_v1,get_mm_mus_label_class\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "SEED = 0\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "K1 = 8192\n",
    "K2=512\n",
    "eps0=0.5\n",
    "D2=512\n",
    "D1=64\n",
    "dir = os.getcwd()\n",
    "ckpt_path_enc = f\"{dir}/outs_encoder_transformer/K{K2}_eps{eps0}_feat_dim{D2}_input_dim128_output_dim{D1//2}_num_layers2_num_heads1_niter50000/seed_0/ckpt_49999.pt\"\n",
    "model_args_enc = TransformerEncoderArgs(\n",
    "            feat_dim=D2,\n",
    "            input_dim=128,\n",
    "            output_dim=D1//2,\n",
    "            num_classes=K2,\n",
    "            num_layers=2,\n",
    "            num_heads=1\n",
    "        )\n",
    "Encoder = TransformerEncoder(model_args_enc)\n",
    "Encoder.load_state_dict(torch.load(ckpt_path_enc), strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2_raw = torch.FloatTensor(mus_class_m2)\n",
    "x2_vit = Encoder.extract_features(x2_raw).squeeze(1)\n",
    "x1 = torch.FloatTensor(mus_class_m1)[mapping_m2_to_m1[:,0]]\n",
    "def l2_norm(v): return v / v.norm(dim=-1, keepdim=True).clamp_min(1e-9)\n",
    "\n",
    "x1_norm      = l2_norm(x1)\n",
    "x2_encoder_proj_norm  = l2_norm(x2_raw)\n",
    "x2_vit_norm = l2_norm(x2_vit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare x2_raw with x2_vit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 Global geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 singular values\n",
      " raw : [44.54999923706055, 44.275001525878906, 44.17100143432617, 43.595001220703125, 43.33700180053711, 43.20899963378906, 42.939998626708984, 42.729000091552734, 42.680999755859375, 42.29999923706055] \n",
      " vit : [37.85599899291992, 34.77799987792969, 33.06999969482422, 32.689998626708984, 31.996999740600586, 31.75, 30.42799949645996, 29.457000732421875, 29.24799919128418, 28.30299949645996]\n",
      "eff-rank raw = 412.1483154296875 \n",
      " vit = 25.776538848876953\n",
      "isotropy raw = 0.43117275834083557 \n",
      " vit = 0.5104237794876099\n"
     ]
    }
   ],
   "source": [
    "# Spectrum\n",
    "Xraw = (x2_raw - x2_raw.mean(0)) / x2_raw.std(0, unbiased=False).clamp_min(1e-9)\n",
    "Xvit = (x2_vit - x2_vit.mean(0)) / x2_vit.std(0, unbiased=False).clamp_min(1e-9)\n",
    "vals_raw = torch.linalg.svdvals(Xraw)\n",
    "vals_vit = torch.linalg.svdvals(Xvit)\n",
    "print(\n",
    "    \"Top-5 singular values\\n raw :\",\n",
    "    (torch.round(vals_raw[:10],  decimals=3)).tolist(),\n",
    "    \"\\n vit :\",\n",
    "    (torch.round(vals_vit[:10],  decimals=3)).tolist()\n",
    ")\n",
    "# Effective rank (lower ⇢ fewer useful dims)\n",
    "def eff_rank(v): \n",
    "    s=v/v.sum()\n",
    "    return torch.exp(-(s*torch.log(s)).sum()) \n",
    "print(\"eff-rank raw =\", eff_rank(vals_raw).item(), \"\\n vit =\", eff_rank(vals_vit).item())\n",
    "\n",
    "# Isotropy ratio (≈trace / λ₁; bigger = flatter)\n",
    "iso = lambda v: v.sum()/(v.max()*len(v)) \n",
    "print(\"isotropy raw =\", iso(vals_raw).item(), \"\\n vit =\", iso(vals_vit).item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 Pair-wise structure (cluster separation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x2_raw_norm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# cosine distance matrix (fits on GPU with K2=512)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     D_raw = \u001b[32m1\u001b[39m - \u001b[43mx2_raw_norm\u001b[49m @ x2_raw_norm.T          \u001b[38;5;66;03m# [K2,K2], 0 means vectors are very similar (angle = 0°), 1 means vectors are orthogonal (angle = 90°)\u001b[39;00m\n\u001b[32m      4\u001b[39m     D_vit = \u001b[32m1\u001b[39m - x2_vit_norm @ x2_vit_norm.T\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstats\u001b[39m(D):\n",
      "\u001b[31mNameError\u001b[39m: name 'x2_raw_norm' is not defined"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # cosine distance matrix (fits on GPU with K2=512)\n",
    "    D_raw = 1 - x2_raw_norm @ x2_raw_norm.T          # [K2,K2], 0 means vectors are very similar (angle = 0°), 1 means vectors are orthogonal (angle = 90°)\n",
    "    D_vit = 1 - x2_vit_norm @ x2_vit_norm.T\n",
    "\n",
    "def stats(D):\n",
    "    tri = D.triu(1)                      # upper triangle, no diag\n",
    "    return tri[tri>0].mean(), tri[tri>0].std()\n",
    "\n",
    "mu_raw,  sd_raw  = stats(D_raw)\n",
    "mu_vit,  sd_vit  = stats(D_vit) \n",
    "print(f\"mean cosine-dist  raw={mu_raw:.3f}±{sd_raw:.3f}   vit={mu_vit:.3f}±{sd_vit:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⟨cos⟩ after Procrustes, raw: 0.17050622403621674 vit: 0.17902471125125885\n"
     ]
    }
   ],
   "source": [
    "from torch.linalg import svd\n",
    "# ❷ Compute cross-covariance and SVD\n",
    "M_raw = x2_raw_norm.T @ x1_norm                          # [D2, D1]\n",
    "U_raw, _, Vt_raw = svd(M_raw, full_matrices=False)   # U:[D2,r], Vt:[r,D1], r = min(D1,D2)\n",
    "M_vit = x2_vit_norm.T @ x1_norm                          # [D2, D1]\n",
    "U_vit, _, Vt_vit = svd(M_vit, full_matrices=False)   # U:[D2,r], Vt:[r,D1], r = min(D1,D2)\n",
    "\n",
    "# ❸ Best orthogonal map W*:      X2 · W* ≈ X1\n",
    "Wstar_raw = U_raw @ Vt_raw                          # [D2, D1]\n",
    "Wstar_vit = U_vit @ Vt_vit                          # [D2, D1]\n",
    "\n",
    "# ❹ Apply and evaluate\n",
    "X2_to_X1_raw = x2_raw_norm @ Wstar_raw                  # [K2, D1]\n",
    "X2_to_X1_vit = x2_vit_norm @ Wstar_vit                  # [K2, D1]\n",
    "paired_cos_raw = torch.sum(x1_norm * X2_to_X1_raw, dim=-1)  # cosine for every class\n",
    "paired_cos_vit = torch.sum(x1_norm * X2_to_X1_vit, dim=-1)  # cosine for every class\n",
    "print(\"⟨cos⟩ after Procrustes, raw:\", paired_cos_raw.mean().item(), \"vit:\", paired_cos_vit.mean().item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of the feature w/wo encoder after projector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the proj only model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MMTransformer(\n",
       "  (layers): ModuleList(\n",
       "    (0-1): 2 x TransformerBlock(\n",
       "      (attn): Attention(\n",
       "        (wq): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (wk): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (wv): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (act): SiLU()\n",
       "      )\n",
       "      (attn_norm): RMSNorm()\n",
       "      (mlp_norm): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       "  (out): Linear(in_features=64, out_features=32, bias=False)\n",
       "  (projector): Projector(\n",
       "    (fc1): Linear(in_features=512, out_features=64, bias=False)\n",
       "    (act): GELU(approximate='none')\n",
       "    (fc2): Linear(in_features=64, out_features=64, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import wandb\n",
    "import torch\n",
    "import numpy as np\n",
    "from model import ModelArgs, Transformer, MMTransformer\n",
    "from dataset import ICLDataset, MMDataset, get_mus_label_class, generate_input_seqs, generate_input_seqs_mm_v1,get_mm_mus_label_class\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "K1 = 8192\n",
    "K2=512\n",
    "eps0=0.5\n",
    "D2=512\n",
    "D1=64\n",
    "N=8\n",
    "model_args_stage2 = ModelArgs(\n",
    "        m1_dim=D1,\n",
    "        m2_dim=D2,\n",
    "        dim=D1,\n",
    "        n_layers=2,\n",
    "        n_heads=1,\n",
    "        n_labels=32,\n",
    "        max_position_embeddings=3*N+1,\n",
    "        rope_theta=10000,\n",
    "        mlp_bias=True,\n",
    "        rms_norm=True,\n",
    "        rope=True,\n",
    "        norm_eps=1e-5,\n",
    "        L_pos=64\n",
    "    )\n",
    "model_stage2 = MMTransformer(model_args_stage2)\n",
    "ckpt_path_stage2 = \"/home/aoq609/ICL/outs_torch/K1_8192_K2_512_N8_D1_64_D2_512_L1_32_L2_16_alpha1_0.0_alpha2_0.0_B2_pB1.0_pC0.0_eps1_0.1_eps2_0.1_no_repeatsFalse_rope_True_rope_theta10000_freeze_layersFalse_n_heads1_n_layers2_rms_normTrue_optimizerSGD_niters80000_n_epochs1/seed_0/ckpt_20000.pt\"\n",
    "model_stage2.load_state_dict(torch.load(ckpt_path_stage2), strict=False)\n",
    "model_stage2.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load stage 3 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLLMTransformer(\n",
      "  (layers): ModuleList(\n",
      "    (0-1): 2 x TransformerBlock(\n",
      "      (attn): Attention(\n",
      "        (wq): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (wk): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (wv): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (rotary_emb): RotaryEmbedding()\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (act): SiLU()\n",
      "      )\n",
      "      (attn_norm): RMSNorm()\n",
      "      (mlp_norm): RMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): RMSNorm()\n",
      "  (out): Linear(in_features=64, out_features=32, bias=False)\n",
      "  (projector): Projector(\n",
      "    (fc1): Linear(in_features=32, out_features=64, bias=False)\n",
      "    (act): GELU(approximate='none')\n",
      "    (fc2): Linear(in_features=64, out_features=64, bias=False)\n",
      "  )\n",
      "  (encoder): TransformerEncoder(\n",
      "    (patch_embed): Linear(in_features=128, out_features=32, bias=True)\n",
      "    (encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-1): 2 x TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=32, out_features=160, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=160, out_features=32, bias=True)\n",
      "          (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (classifier): Linear(in_features=32, out_features=512, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import wandb\n",
    "import torch\n",
    "import numpy as np\n",
    "from model import ModelArgs, MLPEncoderArgs, Transformer, MLPEncoder, MLLMTransformer, TransformerEncoderArgs, TransformerEncoder, CNNEncoderArgs,CNNEncoder\n",
    "from dataset import ICLDataset, MMDataset, get_mus_label_class, generate_input_seqs, generate_input_seqs_mm_v1,get_mm_mus_label_class\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "model_args_mm = ModelArgs(\n",
    "        m1_dim=D1,\n",
    "        m2_dim=D1//2,\n",
    "        dim=D1,\n",
    "        n_layers=2,\n",
    "        n_heads=1,\n",
    "        n_labels=L1,\n",
    "        max_position_embeddings=3*N+1,\n",
    "        rope_theta=rope_theta,\n",
    "        mlp_bias=True,\n",
    "        rms_norm=True,\n",
    "        rope=True,\n",
    "        norm_eps=1e-5,\n",
    "        L_pos=64\n",
    "    )\n",
    "model_args_enc = TransformerEncoderArgs(\n",
    "            feat_dim=D2,\n",
    "            input_dim=128,\n",
    "            output_dim=D1//2,\n",
    "            num_classes=K2,\n",
    "            num_layers=2,\n",
    "            num_heads=1\n",
    "        )\n",
    "Encoder = TransformerEncoder(model_args_enc)\n",
    "K1 = 8192\n",
    "K2=512\n",
    "eps0=0.5\n",
    "D2=512\n",
    "D1=64\n",
    "dir = os.getcwd()\n",
    "ckpt_path_enc = f\"{dir}/outs_encoder_transformer/K{K2}_eps{eps0}_feat_dim{D2}_input_dim128_output_dim{D1//2}_num_layers2_num_heads1_niter50000/seed_0/ckpt_49999.pt\"\n",
    "Encoder.load_state_dict(torch.load(ckpt_path_enc), strict=True)\n",
    "model_stage3 = MLLMTransformer(model_args_mm)\n",
    "model_stage3.init_encoder(Encoder)\n",
    "ckpt_path = \"/home/aoq609/ICL/outs_torch/K1_8192_K2_512_N8_D1_64_D2_512_L1_32_L2_16_alpha1_0.0_alpha2_0.0_B2_pB1.0_pC0.0_eps00.5_eps1_0.1_eps2_0.1_no_repeatsFalse_rope_True_encoder_transformer_freeze_layersFalse_freeze_encoderFalse_n_heads1_n_layers2_niters150000/seed_0/ckpt_140000.pt\"\n",
    "model_stage3.load_state_dict(torch.load(ckpt_path),strict=False)\n",
    "model_stage3.eval()\n",
    "print(model_stage3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 256)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping_m2_to_m1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2_raw = torch.FloatTensor(mus_class_m2)\n",
    "x2_proj = model_stage2.projector(x2_raw)\n",
    "x2_encoder_proj = model_stage3.projector(model_stage3.encoder.extract_features(x2_raw)).squeeze(1)\n",
    "# x1 = torch.FloatTensor(mus_class_m1)[mapping_m2_to_m1[:,0]]\n",
    "mus_class_m1_t = torch.as_tensor(mus_class_m1, dtype=torch.float32)     # [K1, D1]\n",
    "\n",
    "x1 = torch.stack([\n",
    "        mus_class_m1_t[torch.as_tensor(idxs)].mean(dim=0)               # mean along class axis\n",
    "        for idxs in mapping_m2_to_m1                                    # one row per modality-2 class\n",
    "    ])                                                                  # -> [K2, D1]\n",
    "def l2_norm(v): return v / v.norm(dim=-1, keepdim=True).clamp_min(1e-9)\n",
    "\n",
    "x1_norm      = l2_norm(x1).detach()\n",
    "x2_proj_norm = l2_norm(x2_proj).detach()\n",
    "x2_encoder_proj_norm = l2_norm(x2_encoder_proj).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x2_proj.shape torch.Size([512, 64])\n",
      "x2_encoder_proj.shape torch.Size([512, 64])\n",
      "x1.shape torch.Size([512, 64])\n"
     ]
    }
   ],
   "source": [
    "print(\"x2_proj.shape\", x2_proj.shape)\n",
    "print(\"x2_encoder_proj.shape\", x2_encoder_proj.shape)\n",
    "print(\"x1.shape\", x1.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== feature-space similarity to language view ===\n",
      "view                CKA     ⟨cos θ⟩\n",
      "---------------------------------\n",
      "stage-2 proj        0.379   0.567\n",
      "stage-3 enc+proj    0.037   0.493\n"
     ]
    }
   ],
   "source": [
    "import torch, math\n",
    "import numpy as np\n",
    "\n",
    "# ---------- inputs (already L2-normalised) ----------\n",
    "# x1_norm                 : [K2, D1]  language space\n",
    "# x2_proj_norm            : [K2, D?]  stage-2 projector output\n",
    "# x2_encoder_proj_norm    : [K2, D?]  stage-3 encoder+proj output\n",
    "\n",
    "# ---------- 1. linear CKA (feature-space kernel similarity) ----------\n",
    "def linear_cka(X, Y):\n",
    "    \"\"\"\n",
    "    Unbiased linear CKA for centred, L2-normalised matrices.\n",
    "    Works for any matching #rows.\n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "    # centre Gram matrices\n",
    "    H = torch.eye(n, device=X.device) - 1.0 / n\n",
    "    Kc = H @ (X @ X.T) @ H\n",
    "    Lc = H @ (Y @ Y.T) @ H\n",
    "    hsic = (Kc * Lc).sum()\n",
    "    var1 = torch.linalg.norm(Kc) ** 2\n",
    "    var2 = torch.linalg.norm(Lc) ** 2\n",
    "    return (hsic / torch.sqrt(var1 * var2 + 1e-9)).item()\n",
    "\n",
    "# ---------- 2. average principal-angle cosine ----------\n",
    "def subspace_similarity(X, Y, r=None):\n",
    "    \"\"\"\n",
    "    Cosine similarity of principal angles between the column spaces of X and Y.\n",
    "    Returns average of the first r angles (default: min(rank(X), rank(Y))).\n",
    "    \"\"\"\n",
    "    # thin SVD to get orthonormal bases\n",
    "    Ux, _ = torch.linalg.qr(X, mode='reduced')\n",
    "    Uy, _ = torch.linalg.qr(Y, mode='reduced')\n",
    "    # singular values of cross-covariance are cosines of principal angles\n",
    "    S = torch.linalg.svdvals(Ux.T @ Uy)\n",
    "    if r is None:\n",
    "        r = min(len(S), 32)          # look at first 32 by default\n",
    "    return S[:r].mean().item()\n",
    "\n",
    "# ---------- run for both vision views ----------\n",
    "views = {\n",
    "    \"stage-2 proj\"     : x2_proj_norm,\n",
    "    \"stage-3 enc+proj\" : x2_encoder_proj_norm,\n",
    "}\n",
    "\n",
    "print(\"\\n=== feature-space similarity to language view ===\")\n",
    "print(f\"{'view':<18}  CKA     ⟨cos θ⟩\")\n",
    "print(\"-\"*33)\n",
    "for name, V in views.items():\n",
    "    cka  = linear_cka(x1_norm, V)\n",
    "    ang  = subspace_similarity(x1_norm, V, r=32)   # use first 32 principal angles\n",
    "    print(f\"{name:<18}  {cka:5.3f}   {ang:5.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_cosine (stage3) 0.04526278376579285\n",
      "mean_cosine (stage2) -0.08212140202522278\n",
      "top-1 acc  (stage3) 0.001953125\n",
      "top-1 acc  (stage2) 0.001953125\n",
      "CKA (stage3) 0.1439506560564041\n",
      "CKA (stage2) 0.12526308000087738\n"
     ]
    }
   ],
   "source": [
    "import torch, numpy as np\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cross_decomposition import CCA\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "\n",
    "# ---------- 1. helper functions ----------\n",
    "def mean_paired_cosine(a, b):\n",
    "    \"\"\"Cosine similarity of corresponding rows, averaged.\"\"\"\n",
    "    return (a * b).sum(-1).mean().item()\n",
    "\n",
    "def topk_retrieval_accuracy(query, keys, k=1):\n",
    "    \"\"\"\n",
    "    For every vector in `query` find the k nearest neighbours in `keys`\n",
    "    and check whether the ground-truth pair (same row index) is among them.\n",
    "    \"\"\"\n",
    "    # cosine distance == 1 - cosine similarity\n",
    "    dist = pairwise_distances(query.cpu(), keys.cpu(), metric=\"cosine\")\n",
    "    nn     = dist.argsort(axis=1)[:, :k]              # [n, k] indices of nearest keys\n",
    "    target = np.arange(len(query))[:, None]           # ground-truth index per row\n",
    "    hits   = (nn == target).any(axis=1)\n",
    "    return hits.mean().item()\n",
    "\n",
    "def linear_CKA(x, y):\n",
    "    \"\"\"\n",
    "    Closed-form unbiased estimator of linear CKA.\n",
    "    (x, y): [n,d] L2-normalised arrays.\n",
    "    \"\"\"\n",
    "    n = x.shape[0]\n",
    "    hsic = (x @ y.T).pow(2).sum() - x.pow(2).sum() - y.pow(2).sum() + n\n",
    "    var1 = ((x @ x.T).pow(2).sum() - x.pow(2).sum()**2 / n)\n",
    "    var2 = ((y @ y.T).pow(2).sum() - y.pow(2).sum()**2 / n)\n",
    "    return (hsic / torch.sqrt(var1 * var2)).item()\n",
    "\n",
    "# ---------- 2. normalised views ----------\n",
    "# (You already created x1_norm, x2_encoder_proj_norm, x2_proj_norm)\n",
    "\n",
    "# ---------- 3. core metrics ----------\n",
    "metrics = OrderedDict()\n",
    "\n",
    "metrics[\"mean_cosine (stage3)\"]  = mean_paired_cosine(x1_norm, x2_encoder_proj_norm)\n",
    "metrics[\"mean_cosine (stage2)\"] = mean_paired_cosine(x1_norm, x2_proj_norm)\n",
    "\n",
    "metrics[\"top-1 acc  (stage3)\"]   = topk_retrieval_accuracy(x2_encoder_proj_norm,  x1_norm, k=1)\n",
    "metrics[\"top-1 acc  (stage2)\"]  = topk_retrieval_accuracy(x2_proj_norm, x1_norm, k=1)\n",
    "\n",
    "metrics[\"CKA (stage3)\"]          = linear_CKA(x1_norm, x2_encoder_proj_norm)\n",
    "metrics[\"CKA (stage2)\"]         = linear_CKA(x1_norm, x2_proj_norm)\n",
    "\n",
    "for key, value in metrics.items():\n",
    "    print(key, value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2(x2_proj_norm, x1_norm)      = 1.4693\n",
      "L2(x2_encoder_proj_norm, x1_norm) = 1.3760\n",
      "L2(x2_proj_norm, x2_encoder_proj_norm) = 1.4068\n"
     ]
    }
   ],
   "source": [
    "def l2_distance(a, b):\n",
    "    return (a - b).norm(p=2, dim=1).mean().item()\n",
    "\n",
    "dist_proj_vs_x1      = l2_distance(x2_proj_norm, x1_norm)\n",
    "dist_encoder_vs_x1   = l2_distance(x2_encoder_proj_norm, x1_norm)\n",
    "dist_proj_vs_encoder = l2_distance(x2_proj_norm, x2_encoder_proj_norm)\n",
    "\n",
    "print(f\"L2(x2_proj_norm, x1_norm)      = {dist_proj_vs_x1:.4f}\")\n",
    "print(f\"L2(x2_encoder_proj_norm, x1_norm) = {dist_encoder_vs_x1:.4f}\")\n",
    "print(f\"L2(x2_proj_norm, x2_encoder_proj_norm) = {dist_proj_vs_encoder:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ot'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mot\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_wasserstein_distance_pot\u001b[39m(X, Y, p=\u001b[32m2\u001b[39m):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'ot'"
     ]
    }
   ],
   "source": [
    "import ot\n",
    "import torch\n",
    "\n",
    "def compute_wasserstein_distance_pot(X, Y, p=2):\n",
    "    \"\"\"\n",
    "    Compute the p-Wasserstein distance between two empirical distributions\n",
    "    represented by sets of vectors X and Y using POT.\n",
    "\n",
    "    X, Y: [n, d] torch tensors (must be same number of samples)\n",
    "    \"\"\"\n",
    "    X_np = X.cpu().numpy()\n",
    "    Y_np = Y.cpu().numpy()\n",
    "\n",
    "    n = X_np.shape[0]\n",
    "\n",
    "    # Uniform weights (empirical distributions)\n",
    "    a = ot.unif(n)\n",
    "    b = ot.unif(n)\n",
    "\n",
    "    # Cost matrix (squared Euclidean distances)\n",
    "    M = ot.dist(X_np, Y_np, metric='euclidean') ** p\n",
    "\n",
    "    # Solve the optimal transport problem\n",
    "    wasserstein_distance = ot.emd2(a, b, M) ** (1 / p)\n",
    "\n",
    "    return wasserstein_distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
